{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import matplotlib as plt\n",
    "import gensim.downloader\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.corpora.csvcorpus import CsvCorpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from csv import writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news = api.load('word2vec-google-news-300')\n",
    "synonyms = pd.read_csv('synonyms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates pre-trained model's ability to correctly guess the given corpus (in this case synonyms.csv) and returns a pandas dataframe\n",
    "# containing the question word, the correct answer, the model's guess and whether the guess is correct or wrong\n",
    "\n",
    "def model_evaluation(model, corpus):\n",
    "\n",
    "    # Initializes columns for dataframe\n",
    "    question_col, answer_col, guess_col, result_col = [], [], [], []\n",
    "\n",
    "    # vecotrizes the similarity function so it can be applied across the numpy arrays comparing it with the four choices of words\n",
    "    check_similarity = np.vectorize(model.similarity)\n",
    "\n",
    "    # Iterates through each row of the corpus\n",
    "    for index, question in corpus.iterrows():\n",
    "\n",
    "        # extracts the question word, answer word, and choice words\n",
    "        question_word, answer_word, choice_words = question[0], question[1], np.array(question)[2:]\n",
    "\n",
    "        try:\n",
    "            # checks if no choice words are present, if no choice words available - goes to except\n",
    "            if not pd.isnull(choice_words.all()) == 0:\n",
    "                raise KeyError\n",
    "           \n",
    "            # gets an array of the cosine similarities between the question word and the choice words\n",
    "            # and selects word with the highest value\n",
    "            similarities = check_similarity(question_word, choice_words)\n",
    "            guess_word = choice_words[similarities.argmax()]\n",
    "            \n",
    "            # adds question word, answer word, and guess word to their respective column\n",
    "            question_col.append(question_word)\n",
    "            answer_col.append(answer_word)\n",
    "            guess_col.append(guess_word)\n",
    "\n",
    "            # adds correct/wrong to result column\n",
    "            result_col.append('Correct') if guess_word == answer_word else result_col.append('Wrong')\n",
    "            \n",
    "        except KeyError:\n",
    "            # adds question word and answer word to their respective column and adds 'N/A' to guess column as well as 'Guess' to result column\n",
    "            # due to the fact that either the question word and/or all the choice words are not present in the model\n",
    "            question_col.append(question_word)\n",
    "            answer_col.append(answer_word)\n",
    "            guess_col.append('N/A')\n",
    "            result_col.append('Guess')\n",
    "    \n",
    "    return pd.DataFrame({'question word' : question_col, 'answer word': answer_col, 'guess word': guess_col, 'result': result_col})\n",
    "\n",
    "\n",
    "# Gets statistics of correct and wrong guesses from evaluated model\n",
    "def get_stats(model_data):\n",
    "    # extracts the result column from the model_data and counts the amount of correct and wrong guesses\n",
    "    results = pd.value_counts(model_data['result']) \n",
    "    correct_count = results['Correct'] if results.get('Correct') else 0  \n",
    "    wrong_count = results['Wrong'] if results.get('Correct') else 0\n",
    "    guess_count = results['Guess'] if results.get('Correct') else 0\n",
    "    v_statistic= correct_count+wrong_count+guess_count\n",
    "    accuracy = correct_count / v_statistic if v_statistic != 0 else 0\n",
    "    return correct_count, v_statistic, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Statistics and Appending to analysis.csv and google-news-300.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_results = model_evaluation(google_news, synonyms)\n",
    "\n",
    "get_stats(google_news_results)\n",
    "\n",
    "analysis_csv= open(\"analysis.csv\",\"a\")\n",
    "my_writer = writer(analysis_csv)\n",
    "googlenews_model_300_name = \"google-news-300\"\n",
    "googlenews_model_300_name_doc = googlenews_model_300_name + \"-details.csv\"\n",
    "googlenews_model_300_vocabulary_size = len(google_news)\n",
    "googlenews_dataframe = model_evaluation(google_news, synonyms)\n",
    "googlenews_csv = open(googlenews_model_300_name_doc,\"a\")\n",
    "googlenews_dataframe.to_csv(path_or_buf=googlenews_csv, index=False)\n",
    "googlenews_model_300_correct, googlenews_model_300_v_statistic, googlenews_model_300_accuracy = get_stats(googlenews_dataframe)\n",
    "googlenews_model_300_list = [googlenews_model_300_name, googlenews_model_300_vocabulary_size, googlenews_model_300_correct, googlenews_model_300_v_statistic, googlenews_model_300_accuracy]\n",
    "my_writer.writerow(googlenews_model_300_list)\n",
    "analysis_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_info = api.info()\n",
    "# print(available_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 models from different corpora, but same embedding size (Task 2.3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikinews_corpus_300 = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "conceptnet_corpus_300 = api.load(\"conceptnet-numberbatch-17-06-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 models from same corpus, but different embedding sizes (Task 2.3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glovetwitter_corpus_25 = api.load(\"glove-twitter-25\")\n",
    "glovetwitter_corpus_50 = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating statistics and appending to analysis.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_csv= open(\"analysis.csv\",\"a\")\n",
    "my_writer = writer(analysis_csv)\n",
    "wikinews_model_300_name = \"fasttext-wiki-news-subwords-300\"\n",
    "wikinews_model_300_name_doc = wikinews_model_300_name + \"-details.csv\"\n",
    "wikinews_model_300_vocabulary_size = len(wikinews_corpus_300)\n",
    "wikinews_dataframe = model_evaluation(wikinews_corpus_300, synonyms)\n",
    "wikinews_csv = open(wikinews_model_300_name_doc,\"a\")\n",
    "wikinews_dataframe.to_csv(path_or_buf=wikinews_csv, index=False)\n",
    "wikinews_model_300_correct, wikinews_model_300_v_statistic, wikinews_model_300_accuracy = get_stats(wikinews_dataframe)\n",
    "wikinews_model_300_list = [wikinews_model_300_name, wikinews_model_300_vocabulary_size, wikinews_model_300_correct, wikinews_model_300_v_statistic, wikinews_model_300_accuracy]\n",
    "my_writer.writerow(wikinews_model_300_list)\n",
    "\n",
    "conceptnet_model_300_name = \"conceptnet-numberbatch-17-06-300\"\n",
    "conceptnet_model_300_name_doc = conceptnet_model_300_name + \"-details.csv\"\n",
    "conceptnet_model_300_vocabulary_size = len(conceptnet_corpus_300)\n",
    "conceptnet_dataframe = model_evaluation(conceptnet_corpus_300, synonyms)\n",
    "concepnet_csv = open(conceptnet_model_300_name_doc, \"a\")\n",
    "conceptnet_dataframe.to_csv(path_or_buf=concepnet_csv, index=False)\n",
    "conceptnet_model_300_correct, conceptnet_model_300_v_statistic, conceptnet_model_300_accuracy = get_stats(conceptnet_dataframe)\n",
    "conceptnet_model_300_list = [conceptnet_model_300_name, conceptnet_model_300_vocabulary_size, conceptnet_model_300_correct, conceptnet_model_300_v_statistic, conceptnet_model_300_accuracy]\n",
    "my_writer.writerow(conceptnet_model_300_list)\n",
    "\n",
    "glovetwitter_model_25_name = \"glove-twitter-25\"\n",
    "glovetwitter_model_25_name_doc = glovetwitter_model_25_name + \"-details.csv\"\n",
    "glovetwitter_model_25_vocabulary_size = len(glovetwitter_corpus_25)\n",
    "glovetwitter_25_dataframe = model_evaluation(glovetwitter_corpus_25, synonyms)\n",
    "glovetwitter_25_csv = open(glovetwitter_model_25_name_doc,\"a\")\n",
    "glovetwitter_25_dataframe.to_csv(path_or_buf=glovetwitter_25_csv, index=False)\n",
    "glovetwitter_model_25_correct, glovetwitter_model_25_v_statistic, glovetwitter_model_25_accuracy = get_stats(glovetwitter_25_dataframe)\n",
    "glovetwitter_model_25_list = [glovetwitter_model_25_name, glovetwitter_model_25_vocabulary_size, glovetwitter_model_25_correct, glovetwitter_model_25_v_statistic, glovetwitter_model_25_accuracy]\n",
    "my_writer.writerow(glovetwitter_model_25_list)\n",
    "\n",
    "glovetwitter_model_50_name = \"glove-twitter-50\"\n",
    "glovetwitter_model_50_name_doc = glovetwitter_model_50_name + \"-details.csv\"\n",
    "glovetwitter_model_50_vocabulary_size = len(glovetwitter_corpus_50)\n",
    "glovetwitter_50_dataframe = model_evaluation(glovetwitter_corpus_50, synonyms)\n",
    "glovetwitter_50_csv = open(glovetwitter_model_50_name_doc,\"a\")\n",
    "glovetwitter_50_dataframe.to_csv(path_or_buf=glovetwitter_50_csv, index=False)\n",
    "glovetwitter_model_50_correct, glovetwitter_model_50_v_statistic, glovetwitter_model_50_accuracy = get_stats(glovetwitter_50_dataframe)\n",
    "glovetwitter_model_50_list = [glovetwitter_model_50_name, glovetwitter_model_50_vocabulary_size, glovetwitter_model_50_correct, glovetwitter_model_50_v_statistic, glovetwitter_model_50_accuracy]\n",
    "my_writer.writerow(glovetwitter_model_50_list)\n",
    "analysis_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing graphs for every statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"wikinews_vocabulary_size\", \"wikinews_vocabulary_correct\", \"wikinews_vocabulary_guess\", \"wikinews_accuracy\"],[wikinews_model_300_vocabulary_size, wikinews_model_300_vocabulary_correct, wikinews_model_300_vocabulary_guess, wikinews_model_300_accuracy])\n",
    "plt.savefig(\"Wikinews_statistics.pdf\")\n",
    "\n",
    "plt.bar([\"conceptnet_vocabulary_size\", \"conceptnet_vocabulary_correct\", \"conceptnet_vocabulary_guess\", \"conceptnet_accuracy\"],[conceptnet_model_300_vocabulary_size, conceptnet_model_300_vocabulary_correct, conceptnet_model_300_vocabulary_guess, conceptnet_model_300_accuracy])\n",
    "plt.savefig(\"Conceptnet_statistics.pdf\")\n",
    "\n",
    "plt.bar([\"glovetwitter_25_vocabulary_size\", \"glovetwitter_25_vocabulary_correct\", \"glovetwitter_25_vocabulary_guess\", \"glovetwitter_25_accuracy\"],[glovetwitter_model_25_vocabulary_size, glovetwitter_model_25_vocabulary_correct, glovetwitter_model_25_vocabulary_guess, glovetwitter_model_25_accuracy])\n",
    "plt.savefig(\"Glovetwitter_25_statistics.pdf\")\n",
    "\n",
    "plt.bar([\"glovetwitter_50_vocabulary_size\", \"glovetwitter_50_vocabulary_correct\", \"glovetwitter_50_vocabulary_guess\", \"glovetwitter_50_accuracy\"],[glovetwitter_model_50_vocabulary_size, glovetwitter_model_50_vocabulary_correct, glovetwitter_model_50_vocabulary_guess, glovetwitter_model_50_accuracy])\n",
    "plt.savefig(\"Glovetwitter_50_statistics.pdf\")\n",
    "# Mostly done here, waiting on human gold standard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d6f8a830624f0388fba043bda1ab3394ab2d7a44fb0c72262b70fc45ad485ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
